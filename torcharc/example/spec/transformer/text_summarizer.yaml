# attention for text summarizer example
modules:
  embedding:
    Embedding:
      num_embeddings: 10000
      embedding_dim: &embed_dim 128
  # NOTE MultiheadAttention has 2 outputs: output, attn_weights
  self_attn:
    MultiheadAttention:
      embed_dim: *embed_dim
      num_heads: 8
      batch_first: true
  self_attn_output:
    Get:
      key: 0
  cross_attn:
    MultiheadAttention:
      embed_dim: *embed_dim
      num_heads: 8
      batch_first: true
  cross_attn_output:
    Get:
      key: 0
  mlp:
    Sequential:
      - LazyLinear:
          out_features: 64
      - ReLU:
      - LazyLinear:
          out_features: 64
      - LazyLinear:
          out_features: 10000

graph:
  input: [src, tgt]
  modules:
    # reuse
    embedding~src: [src]
    embedding~tgt: [tgt]
    self_attn:
      query: embedding~src
      key: embedding~src
      value: embedding~src
    self_attn_output: [self_attn]
    cross_attn:
      query: embedding~tgt
      key: self_attn_output
      value: self_attn_output
    cross_attn_output: [cross_attn]
    mlp: [cross_attn_output]
  output: mlp
