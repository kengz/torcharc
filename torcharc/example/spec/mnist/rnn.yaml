# MNIST RNN model example from PyTorch https://github.com/pytorch/examples/blob/main/mnist_rnn/main.py
# NOTE this is merely for illustration of functionality; RNN is not practical for MNIST
modules:
  flatten_seq:
    # take (batch, 1, 28, 28) and return (batch, 28, 28)
    Flatten:
      start_dim: 0
      end_dim: 1
  rnn:
    LSTM:
      input_size: 28
      hidden_size: 64
      num_layers: 2
      batch_first: true
  rnn_output:
    # rnn output is tuple; get the first element to pass to mlp
    Get:
      key: 0
  # use torch.narrow to get the last seq element of RNN (batch_size, seq_len, hidden_size) - output[:, -1, :]
  # NOTE this is merely for illustration of functionality
  rnn_seq_last:
    Narrow:
      dim: 1
      start: -1
      length: 1
  # reduce (batch, 1, hidden_size) to (batch, hidden_size)
  flatten_last:
    Flatten:
      start_dim: 1
      end_dim: 2
  mlp:
    Sequential:
      - LazyBatchNorm1d:
      - Dropout2d:
          p: 0.25
      - LazyLinear:
          out_features: 32
      - ReLU:
      - Dropout2d:
          p: 0.5
      - LazyLinear:
          out_features: 10
      - LogSoftmax:
          dim: 1

graph:
  input: image
  modules:
    flatten_seq: [image]
    rnn: [flatten_seq]
    rnn_output: [rnn]
    rnn_seq_last: [rnn_output]
    flatten_last: [rnn_seq_last]
    mlp: [flatten_last]
  output: mlp
